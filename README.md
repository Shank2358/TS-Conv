### æ›´æ–°å•¦~~
# TS-Conv: Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection in Aerial Images  

  <a href="https://github.com/Shank2358/GGHL/">
    <img alt="Version" src="https://img.shields.io/badge/Version-1.3.0-blue" />
  </a>
  
  <a href="https://github.com/Shank2358/GGHL/blob/main/LICENSE">
    <img alt="GPLv3.0 License" src="https://img.shields.io/badge/License-GPLv3.0-blue" />
  </a>
  
  <a href="https://github.com/Shank2358" target="_blank">
  <img src="https://visitor-badge.glitch.me/badge?page_id=gghl.visitor-badge&right_color=blue"
  alt="Visitor" />
</a> 

<a href="mailto:zhanchao.h@outlook.com" target="_blank">
   <img alt="E-mail" src="https://img.shields.io/badge/To-Email-blue" />
</a> 
Code is coming soon.

## This is the implementation of TS-Conv ğŸ‘‹ğŸ‘‹ğŸ‘‹
[[Arxiv](https://arxiv.org/abs/2209.02200)]

### Please give a â­ï¸ if this project helped you. If you use it, please consider citing:
  ```Arxiv
  @ARTICLE{9709203,
  author={Huang, Zhanchao and Li, Wei and Xia, Xiang-Genï¼ŒHao Wang and Tao, Ran},
  journal={arXiv}, 
  title={Task-wise Sampling Convolutions for Arbitrary-Oriented Object Detection in Aerial Images}, 
  year={2022},
  volume={},
  number={},
  pages={1-16},
  doi={10.48550/arXiv.2209.02200}}
  ```
### ğŸ¤¡ğŸ¤¡ğŸ¤¡ Cloneä¸Star,éƒ½æ˜¯è€æµæ°“    


### ğŸ‘¹ğŸ‘¹ğŸ‘¹ ä¸å‡ºæ„å¤–çš„è¯æ˜¯æ¯•ä¸šå‰æœ€åä¸€ä¸ªå·¥ä½œäº†ï¼Œå¯èƒ½ä¹Ÿæ˜¯å­¦æœ¯åœˆçš„æœ€åä¸€ä¸ªå·¥ä½œï¼ˆå³å°†å¤±ä¸šï¼Œï¼Œï¼Œï¼‰ï¼Œæœ‰ç‚¹æ°´å¤§å®¶è§è°…ã€‚   


## 0. Something Important ğŸ¦ ğŸ¦€ ğŸ¦‘ 

* ### ğŸƒğŸƒğŸƒ The usage of the TS-Conv repository is the same as that of the ancestral repository [GGHL](https://github.com/Shank2358/GGHL). If you have any questions, please see the issues there.  
    #### ç”¨æ³•å’Œç¥–ä¼ çš„[GGHL](https://github.com/Shank2358/GGHL)ä»“åº“ä¸€æ ·ï¼Œæœ‰é—®é¢˜å¯ä»¥çœ‹é‚£è¾¹çš„issuesã€‚MMRotateç‰ˆæœ¬ä¹Ÿåœ¨å†™ç€ã€‚TS-Convè¿˜å°†æŒç»­æ›´æ–°ä¸€æ®µæ—¶é—´ï¼Œç°åœ¨æ›´æ–°å®Œçš„æ˜¯ä¸»ä½“æ¨¡å‹çš„ä»£ç ï¼Œé‡ç‚¹åœ¨headï¼ŒDCNï¼Œä»¥åŠdataloadçš„æ ‡ç­¾åˆ†é…é‚£éƒ¨åˆ†ï¼Œå…¶ä»–å’ŒGGHLå·®ä¸å¤šã€‚å¯è§†åŒ–å’Œæ›´å¤šå…¶å®ƒéƒ¨åˆ†çš„åŠŸèƒ½å’Œå®éªŒæˆ‘ä¹Ÿåœ¨æŠ“ç´§æ›´æ–°ä¸­ã€‚   

* ### ğŸ’–ğŸ’–ğŸ’– Thanks to [Crescent-Ao](https://github. com/Crescent-Ao) and [haohaoolalahao](https://github.com/haohaoolalahao) for contributions to the GGHL repository, thanks to [Crescent-Ao](https://github.com/Crescent-Ao) for the GGHL deployment Version. Relevant warehouses will continue to be updated, so stay tuned.  
    #### æ‰“ä¸ªå¹¿å‘Šï¼ŒGGHLéƒ¨ç½²ç‰ˆæœ¬[GGHL-Deployment](https://github.com/Crescent-Ao/GGHL-Deployment)å·²ç»ä¸Šçº¿ï¼Œæ¬¢è¿å¤§å®¶ä½¿ç”¨~~ æ„Ÿè°¢æˆ‘æœ€äº²çˆ±çš„å¸ˆå¼Ÿ[Crescent-Ao](https://github.com/Crescent-Ao)å’Œ[haohaolalahao](https://github.com/haohaolalahao)å¯¹GGHLä»“åº“çš„è´¡çŒ®ï¼Œæ„Ÿè°¢[Crescent-Ao](https://github.com/Crescent-Ao)å®Œæˆçš„GGHLéƒ¨ç½²ç‰ˆæœ¬ã€‚ç›¸å…³ä»“åº“è¿˜ä¼šæŒç»­æ›´æ–°ä¸­ï¼Œæ•¬è¯·æœŸå¾…ã€‚

* ### ğŸ˜ºğŸ˜ºğŸ˜º Welcome everyone to pay attention to the MGAR completed by [haohaoolalahao](https://github.com/haohaoolalahao) in cooperation with me, which has been accepted by [IEEE TGRS](https://ieeexplore.ieee.org/document/9912396).  
    ### å†æ‰“ä¸ªå¹¿å‘Šï¼Œæ¬¢è¿å¤§å®¶å…³æ³¨[haohaolalahao](https://github.com/haohaolalahao)ä¸æˆ‘åˆä½œå®Œæˆçš„é¥æ„Ÿå›¾åƒç›®æ ‡æ£€æµ‹å·¥ä½œ MGAR: Multi-Grained Angle Representation for Remote Sensing Object Detectionï¼Œè®ºæ–‡å·²ç»æ­£å¼æ¥æ”¶[IEEE TGRS](https://ieeexplore.ieee.org/document/9912396) [Arxiv](https://arxiv.org/abs/2209.02884), æ„Ÿè°¢å¤§å®¶å¼•ç”¨ï¼š
  ```IEEE TGRS
    @ARTICLE{9912396,
      author={Wang, Hao and Huang, Zhanchao and Chen, Zhengchao and Song, Ying and Li, Wei},
      journal={IEEE Transactions on Geoscience and Remote Sensing}, 
      title={Multi-Grained Angle Representation for Remote Sensing Object Detection}, 
      year={2022},
      volume={},
      number={},
      pages={1-1},
      doi={10.1109/TGRS.2022.3212592}}
  ```  

## ğŸŒˆ 1.Environments
Linux (Ubuntu 18.04, GCC>=5.4) & Windows (Win10)   
CUDA > 11.1, Cudnn > 8.0.4

First, install CUDA, Cudnn, and Pytorch.
Second, install the dependent libraries in [requirements.txt](https://github.com/Shank2358/GGHL/blob/main/requirements.txt). 

```python
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch 
pip install -r requirements.txt  
```
    
## ğŸŒŸ 2.Installation  
1. git clone this repository    

2. Polygen NMS  
The poly_nms in this version is implemented using shapely and numpy libraries to ensure that it can work in different systems and environments without other dependencies. But doing so will slow down the detection speed in dense object scenes. If you want faster speed, you can compile and use the poly_iou library (C++ implementation version) in datasets_tools/DOTA_devkit. The compilation method is described in detail in [DOTA_devkit](https://github.com/CAPTAIN-WHU/DOTA_devkit) .

```bash
cd datasets_tools/DOTA_devkit
sudo apt-get install swig
swig -c++ -python polyiou.i
python setup.py build_ext --inplace 
```   
  
## ğŸƒ 3.Datasets

1. [DOTA dataset](https://captain-whu.github.io/DOTA/dataset.html) and its [devkit](https://github.com/CAPTAIN-WHU/DOTA_devkit)  

#### (1) Training Format  
You need to write a script to convert them into the train.txt file required by this repository and put them in the ./dataR folder.  
For the specific format of the train.txt file, see the example in the /dataR folder.   

```txt
image_path xmin,ymin,xmax,ymax,class_id,x1,y1,x2,y2,x3,y3,x4,y4,area_ratio,angle[0,180) xmin,ymin,xmax,ymax,class_id,x1,y1,x2,y2,x3,y3,x4,y4,area_ratio,angle[0,180)...
```  
The calculation method of angle is explained in [Issues #1](https://github.com/Shank2358/GGHL/issues/1) and our paper.

#### (2) Validation & Testing Format
The same as the Pascal VOC Format

#### (3) DataSets Files Structure
  ```
  cfg.DATA_PATH = "/opt/datasets/DOTA/"
  â”œâ”€â”€ ...
  â”œâ”€â”€ JPEGImages
  |   â”œâ”€â”€ 000001.png
  |   â”œâ”€â”€ 000002.png
  |   â””â”€â”€ ...
  â”œâ”€â”€ Annotations (DOTA Dataset Format)
  |   â”œâ”€â”€ 000001.txt (class_idx x1 y1 x2 y2 x3 y3 x4 y4)
  |   â”œâ”€â”€ 000002.txt
  |   â””â”€â”€ ...
  â”œâ”€â”€ ImageSets
      â”œâ”€â”€ test.txt (testing filename)
          â”œâ”€â”€ 000001
          â”œâ”€â”€ 000002
          â””â”€â”€ ...
  ```  
There is a DOTA2Train.py file in the datasets_tools folder that can be used to generate training and test format labels.
First, you need to use [DOTA_devkit](https://github.com/CAPTAIN-WHU/DOTA_devkit) , the official tools of the DOTA dataset, for image and label splitting. Then, run DOTA2Train.py to convert them to the format required by GGHL. For the use of DOTA_devkit, please refer to the tutorial in the official repository.

## ğŸŒ ğŸŒ ğŸŒ  4.Usage Example
#### (1) Training  
```bash
sh train_GGHL_dist.sh
```

#### (2) Testing  
```python
python test.py
```
## ğŸ“ License  
Copyright Â© 2021 [Shank2358](https://github.com/Shank2358).<br />
This project is [GNU General Public License v3.0](https://github.com/Shank2358/GGHL/blob/main/LICENSE) licensed.

## ğŸ¤ To be continued 
